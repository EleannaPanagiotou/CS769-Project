{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to .py\n",
    "\n",
    "jupyter nbconvert --to script Helmet_DETR.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124 True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fb12809f1f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting skimage\n",
      "  Using cached skimage-0.0.tar.gz (757 bytes)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[3 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m *** Please install the `scikit-image` package (instead of `skimage`) ***\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First class index: 1\n",
      "Parameter num_classes: 2\n",
      "Fine-tuned classes: ['N/A', 'helmet', 'head']\n"
     ]
    }
   ],
   "source": [
    "first_class_index = 1\n",
    "assert(first_class_index in [0, 1])\n",
    "\n",
    "if first_class_index == 0:\n",
    "\n",
    "  # There is one class, balloon, with ID n°0.\n",
    "\n",
    "  num_classes = 3\n",
    "\n",
    "  finetuned_classes = [\n",
    "      'helmet',\n",
    "      'head',\n",
    "      'person'\n",
    "  ]\n",
    "\n",
    "  # The `no_object` class will be automatically reserved by DETR with ID equal\n",
    "  # to `num_classes`, so ID n°1 here.\n",
    "\n",
    "else:\n",
    "\n",
    "  # There is one class, balloon, with ID n°1.\n",
    "  #\n",
    "  # However, DETR assumes that indexing starts with 0, as in computer science,\n",
    "  # so there is a dummy class with ID n°0.\n",
    "  # Caveat: this dummy class is not the `no_object` class reserved by DETR.\n",
    "\n",
    "  num_classes = 2\n",
    "\n",
    "  finetuned_classes = [\n",
    "      'N/A',  'helmet', 'head'\n",
    "  ]\n",
    "\n",
    "  # The `no_object` class will be automatically reserved by DETR with ID equal\n",
    "  # to `num_classes`, so ID n°2 here.\n",
    "\n",
    "print('First class index: {}'.format(first_class_index))\n",
    "print('Parameter num_classes: {}'.format(num_classes))\n",
    "print('Fine-tuned classes: {}'.format(finetuned_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'detr'...\n",
      "remote: Enumerating objects: 265, done.\u001b[K\n",
      "remote: Total 265 (delta 0), reused 0 (delta 0), pack-reused 265 (from 1)\u001b[K\n",
      "Receiving objects: 100% (265/265), 325.44 KiB | 2.31 MiB/s, done.\n",
      "Resolving deltas: 100% (127/127), done.\n",
      "/home/lefki/AdvancedNLP/DETR_reimplementation/detr/detr\n",
      "Already on 'finetune'\n",
      "Your branch is up to date with 'origin/finetune'.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# %cd /content/\n",
    "!rm -rf detr\n",
    "!git clone https://github.com/woctezuma/detr.git\n",
    "\n",
    "%cd detr/\n",
    "\n",
    "!git checkout finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretrained weights\n",
    "checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',\n",
    "            map_location='cpu',\n",
    "            check_hash=True)\n",
    "\n",
    "# Remove class weights\n",
    "del checkpoint[\"model\"][\"class_embed.weight\"]\n",
    "del checkpoint[\"model\"][\"class_embed.bias\"]\n",
    "\n",
    "# Save\n",
    "torch.save(checkpoint,\n",
    "           '/home/lefki/AdvancedNLP/DETR_reimplementation/detrdetr-r50_no-class-head.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: DETR training and evaluation script [-h] [--lr LR]\n",
      "                                           [--lr_backbone LR_BACKBONE]\n",
      "                                           [--batch_size BATCH_SIZE]\n",
      "                                           [--weight_decay WEIGHT_DECAY]\n",
      "                                           [--epochs EPOCHS]\n",
      "                                           [--lr_drop LR_DROP]\n",
      "                                           [--clip_max_norm CLIP_MAX_NORM]\n",
      "                                           [--frozen_weights FROZEN_WEIGHTS]\n",
      "                                           [--backbone BACKBONE] [--dilation]\n",
      "                                           [--position_embedding {sine,learned}]\n",
      "                                           [--enc_layers ENC_LAYERS]\n",
      "                                           [--dec_layers DEC_LAYERS]\n",
      "                                           [--dim_feedforward DIM_FEEDFORWARD]\n",
      "                                           [--hidden_dim HIDDEN_DIM]\n",
      "                                           [--dropout DROPOUT]\n",
      "                                           [--nheads NHEADS]\n",
      "                                           [--num_queries NUM_QUERIES]\n",
      "                                           [--pre_norm] [--masks]\n",
      "                                           [--no_aux_loss]\n",
      "                                           [--set_cost_class SET_COST_CLASS]\n",
      "                                           [--set_cost_bbox SET_COST_BBOX]\n",
      "                                           [--set_cost_giou SET_COST_GIOU]\n",
      "                                           [--mask_loss_coef MASK_LOSS_COEF]\n",
      "                                           [--dice_loss_coef DICE_LOSS_COEF]\n",
      "                                           [--bbox_loss_coef BBOX_LOSS_COEF]\n",
      "                                           [--giou_loss_coef GIOU_LOSS_COEF]\n",
      "                                           [--eos_coef EOS_COEF]\n",
      "                                           [--num_classes NUM_CLASSES]\n",
      "                                           [--dataset_file DATASET_FILE]\n",
      "                                           [--coco_path COCO_PATH]\n",
      "                                           [--coco_panoptic_path COCO_PANOPTIC_PATH]\n",
      "                                           [--remove_difficult]\n",
      "                                           [--output_dir OUTPUT_DIR]\n",
      "                                           [--device DEVICE] [--seed SEED]\n",
      "                                           [--resume RESUME] [--start_epoch N]\n",
      "                                           [--eval]\n",
      "                                           [--num_workers NUM_WORKERS]\n",
      "                                           [--world_size WORLD_SIZE]\n",
      "                                           [--dist_url DIST_URL]\n",
      "DETR training and evaluation script: error: unrecognized arguments: --ignore_mismatched_sizes=True\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "  --dataset_file \"custom\" \\\n",
    "  --coco_path \"/home/lefki/AdvancedNLP/DETR_reimplementation/processed_dataset\" \\\n",
    "  --output_dir \"outputs\" \\\n",
    "  --resume \"/home/lefki/AdvancedNLP/DETR_reimplementation/detrdetr-r50_no-class-head.pth\" \\\n",
    "  --num_classes $num_classes \\\n",
    "  --epochs 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "model = torch.hub.load('facebookresearch/detr',\n",
    "                       'detr_resnet50',\n",
    "                       pretrained=False,\n",
    "                       num_classes=num_classes)\n",
    "\n",
    "checkpoint = torch.load('/home/lefki/ECE766/helmet_detr/output/checkpoint.pth',\n",
    "                        map_location='cpu')\n",
    "\n",
    "model.load_state_dict(checkpoint['model'],\n",
    "                      strict=False)\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def plot_finetuned_results(pil_img, prob=None, boxes=None):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    if prob is not None and boxes is not None:\n",
    "      for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "          ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                    fill=False, color=c, linewidth=3))\n",
    "          cl = p.argmax()\n",
    "          text = f'{finetuned_classes[cl]}: {p[cl]:0.2f}'\n",
    "          ax.text(xmin, ymin, text, fontsize=15,\n",
    "                  bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def filter_bboxes_from_outputs(outputs,\n",
    "                               threshold=0.7):\n",
    "  \n",
    "  # keep only predictions with confidence above threshold\n",
    "  probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "  keep = probas.max(-1).values > threshold\n",
    "\n",
    "  probas_to_keep = probas[keep]\n",
    "\n",
    "  # convert boxes from [0; 1] to image scales\n",
    "  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
    "  \n",
    "  return probas_to_keep, bboxes_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def run_worflow(my_image, my_model):\n",
    "  # mean-std normalize the input image (batch-size: 1)\n",
    "  img = transform(my_image).unsqueeze(0)\n",
    "\n",
    "  # propagate through the model\n",
    "  outputs = my_model(img)\n",
    "\n",
    "  for threshold in [0.8]:\n",
    "    \n",
    "    probas_to_keep, bboxes_scaled = filter_bboxes_from_outputs(outputs,\n",
    "                                                              threshold=threshold)\n",
    "\n",
    "    plot_finetuned_results(my_image,\n",
    "                           probas_to_keep, \n",
    "                           bboxes_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "\n",
    "from PIL import Image\n",
    "img_name = '/home/lefki/ECE766/helmet_detr/test_images/hard_hat_workers32.png'\n",
    "# img_name = '/home/lefki/ECE766/helmet_detr/test_images/hard_hat_workers84.png'\n",
    "# img_name = '/home/lefki/ECE766/helmet_detr/test_images/hard_hat_workers499.png'\n",
    "# img_name = '/home/lefki/ECE766/helmet_detr/test_images/hard_hat_workers2459.png'\n",
    "# img_name = '/home/lefki/ECE766/helmet_detr/test_images/hard_hat_workers4839.png'\n",
    "im = Image.open(img_name)\n",
    "\n",
    "run_worflow(im,\n",
    "            model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs769",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
